train <- data.table(train)
train = copy(train_orig)
target = train$target
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train$SalePrice = target
train <- data.table(train)
class(target)
train = copy(train_orig)
target = train$SalePrice
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train$SalePrice = target
train <- data.table(train)
train = copy(train_orig)
target = train$SalePrice
class(tagrget)
class(target)
dim(train)
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train[,SalePrice:=NULL]
dim(train)
train <- data.table(train)
train$SalePrice = target
train = read.csv("../RAW/train.csv", header = TRUE)
test = read.csv("../RAW/test.csv", header = TRUE)
source("./lib.R")
train=subset(train,train$GrLivArea<=4000)
train <- treat.missing.values(dat=train)
test <- treat.missing.values(dat=test)
# Binning some numeric variables and dropping the original cols
train$MSSubClass <- as.factor(train$MSSubClass)
test$MSSubClass <- as.factor(test$MSSubClass)
train <- data.table(train)
test <- data.table(test)
train <- bin.variables(dat=train)
test <- bin.variables(dat=test)
train <- misc.features(dat=train)
test <- misc.features(dat=test)
rmv.cols <- c("Street","Condition2", "MiscFeature",
"GarageYrBlt", 'PoolQC','PoolArea', "X1stFlrSF", "YearRemodAdd",
"YearBuilt", "Utilities", 'MoSold','HouseStyle')
dim(train);dim(test)
train[,(rmv.cols):=NULL]
test[,(rmv.cols):=NULL]
any(is.na(train))
any(is.na(test))
colnames(test)[unlist(lapply(colnames(test), function(x){any(is.na(test[,get(x)]))}))]
for(n in names(test_impute$imp)){
x = test_impute$imp[[n]][,1]
test[is.na(get(n)),(n):=x]
}
test$num_ext_materials <- ifelse(as.character(test$Exterior1st)==as.character(test$Exterior2nd),1,2)
train_orig <- copy(train)
target = train$SalePrice
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train <- data.table(train)
train$SalePrice = target
test_orig = copy(test)
dim(train)
test <- predict(dummyObj,test)
dim(train_rmse)
dim(test)
dim(train_orig)
unique(train_orig$MSSubClass)
unique(test$MSSubClass)
library(dplyr)
library(data.table)
library(mice)
setwd("E:/StudyWork/Kaggle/House_Pricing/CODES")
train = read.csv("../RAW/train.csv", header = TRUE)
test = read.csv("../RAW/test.csv", header = TRUE)
source("./lib.R")
train=subset(train,train$GrLivArea<=4000)
train <- treat.missing.values(dat=train)
test <- treat.missing.values(dat=test)
# Binning some numeric variables and dropping the original cols
train$MSSubClass <- as.factor(train$MSSubClass)
test$MSSubClass <- as.factor(test$MSSubClass)
train <- data.table(train)
test <- data.table(test)
train <- bin.variables(dat=train)
test <- bin.variables(dat=test)
train <- misc.features(dat=train)
test <- misc.features(dat=test)
rmv.cols <- c("Street","Condition2", "MiscFeature",
"GarageYrBlt", 'PoolQC','PoolArea', "X1stFlrSF", "YearRemodAdd",
"YearBuilt", "Utilities", 'MoSold','HouseStyle')
dim(train);dim(test)
train[,(rmv.cols):=NULL]
test[,(rmv.cols):=NULL]
any(is.na(train))
any(is.na(test))
colnames(test)[unlist(lapply(colnames(test), function(x){any(is.na(test[,get(x)]))}))]
test_impute <- mice(test[,c("MSZoning","Exterior1st","Exterior2nd","Functional",
"SaleType"), with=F], m=1,  method = "cart",
seed=123, maxit = 100, printFlag = F )
for(n in names(test_impute$imp)){
x = test_impute$imp[[n]][,1]
test[is.na(get(n)),(n):=x]
}
test$num_ext_materials <- ifelse(as.character(test$Exterior1st)==as.character(test$Exterior2nd),1,2)
train_orig <- copy(train)
target = train$SalePrice
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train <- data.table(train)
train$SalePrice = target
library(caret)
dim(train)
dim(test)
train_orig <- copy(train)
target = train$SalePrice
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train <- data.table(train)
train$SalePrice = target
dim(train)
model=lm(SalePrice~.,train)
cols_to_keep <- gsub("`","",rownames(summary(model)$coeff))
cols_to_keep <- cols_to_keep[-1]
train <- subset(train, select = c(cols_to_keep,"SalePrice"))
set.seed(123)
indx = sample(1:nrow(train),0.3*nrow(train),replace = F)
val <- train[indx,]
train <- train[-indx,]
dim(tarin)
dim(train)
library(glmnet)
y_train <- log(train[["SalePrice"]]+1)
x_train = copy(train)
x_train[,":="(SalePrice=NULL, Id =NULL)]
y_val <- log(val[["SalePrice"]]+1)
x_val = copy(val)
x_val[,":="(SalePrice=NULL, Id =NULL)]
grid=seq(1,0,-0.001)
set.seed(1)
ridge.mod=glmnet(as.matrix(x_train),y_train,alpha=1, lambda =grid)
cv.out=cv.glmnet(as.matrix(x_train),y_train,alpha=1)
ridge.pred_test=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_val))
ridge.pred_train=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_train))
test_rmse_ridge=RMSE(y_val,ridge.pred_test,wt=1)
train_rmse_ridge=RMSE(y_train,ridge.pred_train,wt=1)
print(test_rmse_ridge)
print(train_rmse_ridge)
bestlam =cv.out$lambda.min #0.09013617
ridge.pred_test=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_val))
ridge.pred_train=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_train))
test_rmse_ridge=RMSE(y_val,ridge.pred_test,wt=1)
train_rmse_ridge=RMSE(y_train,ridge.pred_train,wt=1)
print(test_rmse_ridge)
print(train_rmse_ridge)
train = copy(train_orig)
target = train$target
train[,target:=NULL]
dim(train)
target = train$SalePrice
train[,SalePrice:=NULL]
dat_all <- rbind(train,test)
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
fm <- formula(paste("~ ",paste(colnames(dat_all),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = dat_all, sep = NULL)
dat_all <- predict(dummyObj,dat_all)
train <- dat_all[1:nrow(train_orig),]
test <- dat_all[(nrow(train_orig)+1):nrow(dat_all)]
dim(train)
dim(test)
dim(dat_all)
test <- dat_all[(nrow(train_orig)+1):nrow(dat_all),]
dim(test)
train <- data.table(train)
test <- data.table(test)
train$SalePrice = target
model=lm(SalePrice~.,train)
cols_to_keep <- gsub("`","",rownames(summary(model)$coeff))
cols_to_keep <- cols_to_keep[-1]
train <- subset(train, select = c(cols_to_keep,"SalePrice"))
dim(train)
y_train <- log(train[["SalePrice"]]+1)
x_train = copy(train)
x_train[,":="(SalePrice=NULL, Id =NULL)]
grid=seq(1,0,-0.001)
set.seed(1)
ridge.mod=glmnet(as.matrix(x_train),y_train,alpha=1, lambda =grid)
cv.out=cv.glmnet(as.matrix(x_train),y_train,alpha=1)
#plot(cv.out)
bestlam =cv.out$lambda.min
class(test)
x_test = copy(test)
x_test <- subset(x_test, select = colnames(x_train))
x_test[,Id:= NULL]
ridge.pred_test=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_test))
test$SalePrice =exp(ridge.pred_test[,1]) - 1
write.csv(test[,c("Id","SalePrice"),with=F], file = "../MODEL/Submission_V1.csv")
write.csv(test[,c("Id","SalePrice"),with=F], file = "../MODEL/Submission_V1.csv", row.names = F)
any(is.na(x_test))
colnames(x_test)[unlist(lapply(colnames(x_test), function(x) any(is.na(x_test[,get(x)])))]
colnames(x_test)[unlist(lapply(colnames(x_test), function(x) any(is.na(x_test[,get(x)]))))]
which(is.na(x_test$BsmtFinSF1))
which(is.na(x_test$BsmtFinSF2))
which(is.na(x_test$BsmtUnfSF))
train = read.csv("../RAW/train.csv", header = TRUE)
test = read.csv("../RAW/test.csv", header = TRUE)
source("./lib.R")
train=subset(train,train$GrLivArea<=4000)
train <- treat.missing.values(dat=train)
test <- treat.missing.values(dat=test)
# Binning some numeric variables and dropping the original cols
train$MSSubClass <- as.factor(train$MSSubClass)
test$MSSubClass <- as.factor(test$MSSubClass)
train <- data.table(train)
test <- data.table(test)
train <- bin.variables(dat=train)
test <- bin.variables(dat=test)
train <- misc.features(dat=train)
test <- misc.features(dat=test)
rmv.cols <- c("Street","Condition2", "MiscFeature",
"GarageYrBlt", 'PoolQC','PoolArea', "X1stFlrSF", "YearRemodAdd",
"YearBuilt", "Utilities", 'MoSold','HouseStyle')
dim(train);dim(test)
train[,(rmv.cols):=NULL]
test[,(rmv.cols):=NULL]
any(is.na(train))
any(is.na(test))
colnames(test)[unlist(lapply(colnames(test), function(x){any(is.na(test[,get(x)]))}))]
test_impute <- mice(test[,c("MSZoning","Exterior1st","Exterior2nd","Functional",
"SaleType"), with=F], m=1,  method = "cart",
seed=123, maxit = 100, printFlag = F )
for(n in names(test_impute$imp)){
x = test_impute$imp[[n]][,1]
test[is.na(get(n)),(n):=x]
}
test$num_ext_materials <- ifelse(as.character(test$Exterior1st)==as.character(test$Exterior2nd),1,2)
any(is.na(train))
any(is.na(test))
colnames(x_test)[unlist(lapply(colnames(x_test), function(x) any(is.na(x_test[,get(x)]))))]
colnames(test)[unlist(lapply(colnames(test), function(x) any(is.na(test[,get(x)]))))]
train = read.csv("../RAW/train.csv", header = TRUE)
test = read.csv("../RAW/test.csv", header = TRUE)
source("./lib.R")
train=subset(train,train$GrLivArea<=4000)
train <- treat.missing.values(dat=train)
test <- treat.missing.values(dat=test)
# Binning some numeric variables and dropping the original cols
train$MSSubClass <- as.factor(train$MSSubClass)
test$MSSubClass <- as.factor(test$MSSubClass)
train <- data.table(train)
test <- data.table(test)
train <- bin.variables(dat=train)
test <- bin.variables(dat=test)
train <- misc.features(dat=train)
test <- misc.features(dat=test)
rmv.cols <- c("Street","Condition2", "MiscFeature",
"GarageYrBlt", 'PoolQC','PoolArea', "X1stFlrSF", "YearRemodAdd",
"YearBuilt", "Utilities", 'MoSold','HouseStyle')
dim(train);dim(test)
train[,(rmv.cols):=NULL]
test[,(rmv.cols):=NULL]
any(is.na(train))
any(is.na(test))
colnames(test)[unlist(lapply(colnames(test), function(x){any(is.na(test[,get(x)]))}))]
for(n in names(test_impute$imp)){
x = test_impute$imp[[n]][,1]
test[is.na(get(n)),(n):=x]
}
test$num_ext_materials <- ifelse(as.character(test$Exterior1st)==as.character(test$Exterior2nd),1,2)
any(is.na(test))
train_orig <- copy(train)
target = train$SalePrice
train[,SalePrice:=NULL]
dat_all <- rbind(train,test)
fm <- formula(paste("~ ",paste(colnames(dat_all),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = dat_all, sep = NULL)
dat_all <- predict(dummyObj,dat_all)
dat_all <- data.table(dat_all)
train <- dat_all[1:nrow(train_orig),]
test <- dat_all[(nrow(train_orig)+1):nrow(dat_all),]
dim(train)
any(is.na(train))
any(is.na(test))
train <- data.table(train)
test <- data.table(test)
train$SalePrice = target
model=lm(SalePrice~.,train)
cols_to_keep <- gsub("`","",rownames(summary(model)$coeff))
cols_to_keep <- cols_to_keep[-1]
train <- subset(train, select = c(cols_to_keep,"SalePrice"))
y_train <- log(train[["SalePrice"]]+1)
x_train = copy(train)
x_train[,":="(SalePrice=NULL, Id =NULL)]
grid=seq(1,0,-0.001)
set.seed(1)
ridge.mod=glmnet(as.matrix(x_train),y_train,alpha=1, lambda =grid)
cv.out=cv.glmnet(as.matrix(x_train),y_train,alpha=1)
#plot(cv.out)
bestlam =cv.out$lambda.min
x_test = copy(test)
x_test <- subset(x_test, select = colnames(x_train))
x_test[,Id:= NULL]
any(is.na(x_test))
ridge.pred_test=predict (ridge.mod ,s=bestlam ,newx=as.matrix(x_test))
test$SalePrice =exp(ridge.pred_test[,1]) - 1
write.csv(test[,c("Id","SalePrice"),with=F], file = "../MODEL/Submission_V1.csv", row.names = F)
summary(model)
dim(train_orig)
train <- copy(train_orig)
dim(test)
target = train$SalePrice
train[,SalePrice:=NULL]
fm <- formula(paste("~ ",paste(colnames(train),collapse = "+"), sep = ""))
dummyObj = dummyVars(formula = fm, data = train, sep = NULL)
train <- predict(dummyObj,train)
train <- data.table(train)
train$SalePrice = target
model=lm(SalePrice~.,train)
cols_to_keep <- gsub("`","",rownames(summary(model)$coeff))
cols_to_keep <- cols_to_keep[-1]
train <- subset(train, select = c(cols_to_keep,"SalePrice"))
set.seed(123)
indx = sample(1:nrow(train),0.3*nrow(train),replace = F)
val <- train[indx,]
train <- train[-indx,]
xgb_grid = expand.grid(nrounds = c(100,300,500,100),
eta = c(0.001, 0.005, 0.008, 0.01),
max_depth = c(5),
colsample_bytree = c(0.5,0.75,0.9,1),
min_child_weight = c(2,3,5,10))
library(xgboost)
y_train <- log(train[["SalePrice"]]+1)
x_train = copy(train)
x_train[,":="(SalePrice=NULL, Id =NULL)]
y_val <- log(val[["SalePrice"]]+1)
x_val = copy(val)
x_val[,":="(SalePrice=NULL, Id =NULL)]
colnames(x_train)
dim(xgb_grid)
xgb_grid = expand.grid(nrounds = c(100,300,500,1000),
eta = c(0.001, 0.005, 0.008, 0.01),
max_depth = c(5),
colsample_bytree = c(0.5,0.75,0.9,1),
min_child_weight = c(2,3,5,10))
param <- list(max_depth = 5,
eta = 0.01,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
train.xg <- xgb.DMatrix(as.matrix((x_train)), label=y_train, missing=111222333)
test.xg <- xgb.DMatrix(as.matrix((x_val)), label=y_val, missing=111222333)
watchlist <- list(test = test.xg,train = train.xg)
model_xgb <- xgb.train(data=train.xg, nrounds = 1500,
params = param, verbose = 2, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = T)
preds_xgb = predict(model_xgb,newdata = as.matrix(x_val))
rmse_xgb = RMSE(y_val,preds_xgb,wt=1)
print(rmse_xgb)
summary(y_train)
summary(y_val)
model_xgb <- xgb.train(data=train.xg, nrounds = 1500,
params = param, verbose = 2, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F)
preds_xgb = predict(model_xgb,newdata = as.matrix(x_val))
rmse_xgb = RMSE(y_val,preds_xgb,wt=1)
print(rmse_xgb)
train = copy(train_orig)
dim(train)
set.seed(123)
indx = sample(1:nrow(train),0.3*nrow(train),replace = F)
val <- train[indx,]
train <- train[-indx,]
drop.cols = c("BsmtCond","ExterCond","GarageCond")
train <- subset(train,select = !(colnames(train)%in% drop.cols))
val <- subset(val,select = !(colnames(val)%in% drop.cols))
y_train <- log(train[["SalePrice"]]+1)
x_train = copy(train)
cols.fac <- names(which(sapply(train,class)=="factor"))
x_train <- x_train[,(cols.fac):=lapply(.SD,function(x) as.numeric(x)), .SDcols=cols.fac]
x_train[,":="(SalePrice=NULL, Id =NULL)]
y_val <- log(val[["SalePrice"]]+1)
x_val = copy(val)
x_val <- x_val[,(cols.fac):=lapply(.SD,function(x) as.numeric(x)), .SDcols=cols.fac]
x_val[,":="(SalePrice=NULL, Id =NULL)]
param <- list(max_depth = 5,
eta = 0.01,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
train.xg <- xgb.DMatrix(as.matrix((x_train)), label=y_train, missing=111222333)
test.xg <- xgb.DMatrix(as.matrix((x_val)), label=y_val, missing=111222333)
watchlist <- list(test = test.xg,train = train.xg)
model_xgb <- xgb.train(data=train.xg, nrounds = 1500,
params = param, verbose = 2, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F)
param <- list(max_depth = 8,
eta = 0.01,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
dim(x_test)
dim(x_val)
model_xgb <- xgb.train(data=train.xg, nrounds = 1500,
params = param, verbose = 2, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F)
?xgb.train
param <- list(max_depth = 5,
eta = 0.001,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
model_xgb <- xgb.train(data=train.xg, nrounds = 1500,
params = param, verbose = 2, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F)
param <- list(max_depth = 5,
eta = 0.001,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
model_xgb <- xgb.train(data=train.xg, nrounds = 5000,
params = param, verbose = 0, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F)
param <- list(max_depth = 5,
eta = 0.1,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
model_xgb <- xgb.train(data=train.xg, nrounds = 5000,
params = param, verbose = 0, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F, print.every.n = 100)
imp_xgb = xgb.importance(model = model_xgb, feature_names = colnames(x_train))
imp_xgb
options(scipen = 999)
imp_xgb
write.csv(imp_xgb, file = "../MODEL/imp_xgb.csv", row.names=F)
param <- list(max_depth = 15,
eta = 0.07,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
model_xgb <- xgb.train(data=train.xg, nrounds = 5000,
params = param, verbose = 0, missing = 111222333,
early.stop.round = 50,
watchlist = watchlist,
maximize = F, print.every.n = 100)
param <- list(max_depth = 4,
eta = 0.07,
silent = 1,
objective="reg:linear",
eval_metric="rmse",
# subsample = 0.75,
min_child_weight = 2,
colsample_bytree = 0.5,
base_score =0)
model_xgb <- xgb.train(data=train.xg, nrounds = 5000,
params = param, verbose = 0, missing = 111222333,
early.stop.round = 500,
watchlist = watchlist,
maximize = F, print.every.n = 100)
